{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on small amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Comments\n",
    "\n",
    "The data is scattered in .txt files all over the positive and the negative directory.  \n",
    "Therefore, we would **read all the .txt files** and **assign labels** to them.   \n",
    "Finally, **save to a Pandas DataFrame** for convenience.\n",
    "\n",
    "### Dataset -- Booking.com comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b04305ccb8b46eba02a5f7f101a00a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=196401), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bc549931b5416d9e1e8bbbca272a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=145321), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read comments in the directory\n",
    "def read_dir(directory):\n",
    "    directory\n",
    "    content_list = []\n",
    "    files = os.listdir(directory)\n",
    "    for file in tqdm(files):\n",
    "        with open(os.path.join(directory, file), 'r', encoding='utf8') as f:\n",
    "            content_list.append(f.read().strip())\n",
    "    return content_list\n",
    "            \n",
    "\n",
    "positives = read_dir('./dataset/CrawlerToBooking/positiveReviews/')\n",
    "negatives = read_dir('./dataset/CrawlerToBooking/negativeReviews/')\n",
    "\n",
    "# Add label on data and Transfer to Dataframe for convenience\n",
    "booking_dataset = pd.DataFrame({'TEXT': positives + negatives, \n",
    "                               'SENTI': [1]*len(positives) + [0]*len(negatives)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as a CSV File\n",
    "\n",
    "Because the **line break** will lead the unexpected newline in .csv file.  \n",
    "We deal with this problem by **substitute a blank** for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 乾淨不受打擾。\n",
      "沒有櫃台人員是十分信認顧客的地方，雖然或許少了點噓寒問暖的親切，卻也多了一份不受監視的自在感。 \n",
      "After: 乾淨不受打擾。 沒有櫃台人員是十分信認顧客的地方，雖然或許少了點噓寒問暖的親切，卻也多了一份不受監視的自在感。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Substitute blank for line break\n",
    "def sub_newline(x): return re.sub('\\\\n', ' ', x)\n",
    "print('Before: %s \\nAfter: %s' % (booking_dataset.TEXT[0], sub_newline(booking_dataset.TEXT[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>SENTI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>乾淨不受打擾。 沒有櫃台人員是十分信認顧客的地方，雖然或許少了點噓寒問暖的親切，卻也多了一份...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>環境整潔乾淨 管家親切</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>位置很好，旁邊就是早市，早上很熱鬧且有許多好吃的早餐在附近。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>喜歡整棟樓的香味包括房間...房間的舒適度真的很好會讓人想再進去關顧！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>路邊就可以停車，住起來還算不錯~價格也不貴~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  SENTI\n",
       "0  乾淨不受打擾。 沒有櫃台人員是十分信認顧客的地方，雖然或許少了點噓寒問暖的親切，卻也多了一份...      1\n",
       "1                                        環境整潔乾淨 管家親切      1\n",
       "2                     位置很好，旁邊就是早市，早上很熱鬧且有許多好吃的早餐在附近。      1\n",
       "3                喜歡整棟樓的香味包括房間...房間的舒適度真的很好會讓人想再進去關顧！      1\n",
       "4                             路邊就可以停車，住起來還算不錯~價格也不貴~      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove line break to avoid wrong in .csv file\n",
    "booking_dataset['TEXT'] = booking_dataset['TEXT'].map(sub_newline)\n",
    "booking_dataset.to_csv('./dataset/booking.csv', index=False, encoding='utf8')\n",
    "\n",
    "# Display\n",
    "booking_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV File\n",
    "\n",
    "Read the .csv file to a Dataframe.  \n",
    "Pandas would regard a comment **composed of only numbers** as **float-type**.  \n",
    "Hence, we need to transfer it into **string-type** manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_dataset = pd.read_csv('./dataset/booking.csv', encoding='utf8')\n",
    "booking_dataset['TEXT'] = booking_dataset['TEXT'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Data\n",
    "\n",
    "Briefly check **the lengths** of most of the comments.  \n",
    "Find a suitable split point to apply BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    341722.000000\n",
       "mean         24.391110\n",
       "std          35.348509\n",
       "min           1.000000\n",
       "25%           7.000000\n",
       "50%          14.000000\n",
       "75%          28.000000\n",
       "max        1973.000000\n",
       "Name: TEXT, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "text_len = booking_dataset['TEXT'].map(len)\n",
    "text_len.plot.hist(bins=300, density=True, cumulative=True, histtype='step', range=(0, 110))\n",
    "plt.show()\n",
    "\n",
    "text_len.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the cumulative percentage at 80%, 85%, 90%, 92%, and 95%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative %   # Words  # Comments\n",
      " 80.35245   \t  33 \t    2582\n",
      " 85.27370   \t  41 \t    1808\n",
      " 90.22685   \t  54 \t    1020\n",
      " 92.17727   \t  62 \t    739\n",
      " 95.04656   \t  80 \t    419\n"
     ]
    }
   ],
   "source": [
    "max_len = text_len.max()\n",
    "\n",
    "len_sum = [0] * max_len\n",
    "for i in text_len:\n",
    "    len_sum[i-1] += 1\n",
    "    \n",
    "len_cum = [len_sum[0]] + [0] * (max_len-1)\n",
    "for i in range(1, max_len):\n",
    "    len_cum[i] += len_sum[i] + len_cum[i-1]\n",
    "\n",
    "print('Cumulative %   # Words  # Comments')\n",
    "for i in range(max_len):\n",
    "    len_cum[i] /= len(text_len)\n",
    "    if len_sum[i] != 0:\n",
    "        if (len_cum[i] >= 0.8 and len_cum[i-1] < 0.8):\n",
    "            print(' %.5f   \\t  %d \\t    %d'%(len_cum[i]*100, i, len_sum[i]))\n",
    "        if (len_cum[i] >= 0.85 and len_cum[i-1] < 0.85):\n",
    "            print(' %.5f   \\t  %d \\t    %d'%(len_cum[i]*100, i, len_sum[i]))\n",
    "        if (len_cum[i] >= 0.9 and len_cum[i-1] < 0.9):\n",
    "            print(' %.5f   \\t  %d \\t    %d'%(len_cum[i]*100, i, len_sum[i]))\n",
    "        if (len_cum[i] >= 0.92 and len_cum[i-1] < 0.92):\n",
    "            print(' %.5f   \\t  %d \\t    %d'%(len_cum[i]*100, i, len_sum[i]))\n",
    "        if (len_cum[i] >= 0.95 and len_cum[i-1] < 0.95):\n",
    "            print(' %.5f   \\t  %d \\t    %d'%(len_cum[i]*100, i, len_sum[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally choose **60** as the Max Length. (≒ 91%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "1. Data cleaning\n",
    "2. Remove too long comments\n",
    "3. Sampling training (2%) / testing (10%) / validation set (10%)\n",
    "4. Save to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Training samples: 6254\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>SENTI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>311944</td>\n",
       "      <td>櫃台只有兩個人，還要顧超商，chck in超過20分，四人房只有兩人份的用品，電視新聞台一直...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218031</td>\n",
       "      <td>沒有</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37266</td>\n",
       "      <td>很酷的無人飯店</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164174</td>\n",
       "      <td>星巴克早餐送進房間, 看海吃早餐, 超棒.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81727</td>\n",
       "      <td>早餐很特別，好吃!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     TEXT  SENTI\n",
       "311944  櫃台只有兩個人，還要顧超商，chck in超過20分，四人房只有兩人份的用品，電視新聞台一直...      0\n",
       "218031                                                 沒有      0\n",
       "37266                                             很酷的無人飯店      1\n",
       "164174                              星巴克早餐送進房間, 看海吃早餐, 超棒.      1\n",
       "81727                                         早餐很特別，好吃!!!      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleaning -- Remove some empty comments.\n",
    "empty = ((booking_dataset['TEXT'].isnull()) | \\\n",
    "         (booking_dataset['TEXT'] == '')    | \\\n",
    "         (booking_dataset['TEXT'] == '0')   | \\\n",
    "         (booking_dataset['TEXT'].map(len) == 0))\n",
    "train_set = booking_dataset[~empty]\n",
    "\n",
    "# Exclude the comments too long to avoid the whole sequence encoding by BERT out of GPU memory\n",
    "MAX_LENGTH = 60\n",
    "train_set = train_set[~(train_set.TEXT.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "\n",
    "# Take 10% of data as test set; Another 10% of data as valid set.\n",
    "SAMPLE_FRAC = 0.2\n",
    "df_test = train_set.sample(frac=SAMPLE_FRAC, random_state=9527)\n",
    "\n",
    "df_train = train_set.loc[~train_set.index.isin(df_test.index)]\n",
    "\n",
    "df_valid = df_test.sample(frac=0.5, random_state=9527)\n",
    "df_test = df_test.loc[~df_test.index.isin(df_valid.index)]\n",
    "\n",
    "\n",
    "# Just take 2% of data as training set, look how much effect BERT could make on small labeled data.\n",
    "SAMPLE_CNT = int(0.02 * len(train_set))\n",
    "df_train = df_train.sample(n=SAMPLE_CNT, random_state=9527)\n",
    "\n",
    "\n",
    "# Save the processed result to a tsv for Pytorch.\n",
    "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "df_valid.to_csv(\"dev.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"# of Training samples:\", len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of positive and negative instances in every set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset: [0.57918014 0.42081986]\n",
      "train set: [0.57467221 0.42532779]\n",
      "test set: [0.58290429 0.41709571]\n",
      "dev set: [0.58152921 0.41847079]\n",
      "# of testing samples： 31271\n"
     ]
    }
   ],
   "source": [
    "# original proportion\n",
    "print('original dataset:', train_set.SENTI.value_counts().values / len(train_set))\n",
    "\n",
    "# train proportion\n",
    "print('train set:', df_train.SENTI.value_counts().values / len(df_train))\n",
    "\n",
    "# test proportion\n",
    "print('test set:', df_test.SENTI.value_counts().values / len(df_test))\n",
    "\n",
    "# valid proportion\n",
    "print('dev set:', df_valid.SENTI.value_counts().values / len(df_valid))\n",
    "\n",
    "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"# of testing samples：\", len(df_test))\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparing -- Dataset\n",
    "\n",
    "First, Load BERT pre-trained model -- bert-base-chinese.  \n",
    "We write a dataset proccessor **CommentDataset** to process each comment to an id tensor, position tensor, label tensor.  \n",
    "That is the basic input format for BERT.\n",
    "\n",
    "### What is CommentDataset doing?\n",
    "1. Take BERT tokenizer to split the comment into tokens. Add **[CLS]** before and **[SEP]** after the sequence.  \n",
    "2. Then transfer the sequence into corresponding indices by a dictionary pre-defined by BERT model.  \n",
    "3. Setting the position sequences to all 0s.  \n",
    "4. Convert the id sequence, position sequence and label into tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # Chinese BERT pre-trained model\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "    \n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\", \"dev\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer               # BERT tokenizer\n",
    "    \n",
    "    \n",
    "    # Define the function that returns a training / testing data\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"k\":\n",
    "            text = self.df.iloc[idx, :1].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text, label = self.df.iloc[idx, :2].values\n",
    "            # Convert the label into tensor\n",
    "            label_tensor = torch.tensor(label)\n",
    "            \n",
    "        # Build BERT tokens of a sentence, and add the delimiter [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens + [\"[SEP]\"]\n",
    "        length = len(word_pieces)\n",
    "        \n",
    "        # Transfer all the token sequence into indices sequence.\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # Set the position sequence as all 0s representing the sentence containing [SEP]\n",
    "        segments_tensor = torch.tensor([0] * length, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# Init a Dataset for loading training data, and tokenized by BERT model.\n",
    "trainset = CommentDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Preparing -- Dataloader\n",
    "\n",
    "After building Dataset, we need a loader to **load batch data** when model applying.  \n",
    "In this step, we consider **inconsistent length of data** when **parallel operation on GPU**.  \n",
    "Hence, we do **zero-padding** to some input tensors which is inconsistent with others.  \n",
    "And mark the valid part (not produced by padding) and invalid part, for model to consult.  \n",
    "By the way, the mark usually used to **attention mechanism**.\n",
    "\n",
    "\n",
    "### What is create_mini_batch do?\n",
    "The input \"samples\" is a list where each element is a sample returned by the CommentDataset we just defined.  \n",
    "Each sample contains 3 tensors：\n",
    "- tokens_tensor\n",
    "- segments_tensor\n",
    "- label_tensor  \n",
    "\n",
    "The function will do **zero padding** on the first 2 tensors, and generate a **masks_tensors** for other uses such as  attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # Testing set has no label.\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # Pad zeros after the sequence into the same length\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    # Attention masks\n",
    "    # Set 1 to the positions that are not produced by padding in tokens_tensor.\n",
    "    #  As a result, BERT will pay attention on the tokens with position mask = 1\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# Init a Dataloader returning 8 training samples per batch\n",
    "BATCH_SIZE = 8\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the validation and testing Dataloader\n",
    "devset = CommentDataset(\"dev\", tokenizer=tokenizer)\n",
    "devloader = DataLoader(devset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "\n",
    "testset = CommentDataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Input a model and a set of data.  \n",
    "Return the predict result and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            # Move all the tensors to GPU\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            # \n",
    "            # The order of tensors are tokens, segments and masks\n",
    "            # Had better assign them to the corresponding parameter names when applying model.\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # Calc the accuracy of training set\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # Record the predictions of current batch.\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "            \n",
    "            del data\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Check whether device is GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict by init classifier\n",
    "\n",
    "Run the model on device (GPU), and get the accuracy of training set.  \n",
    "Move model out of GPU to prevent model from taking up too much resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 2\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9873695a0b14c488ea4d8c4ddaa2551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "classification acc: 0.5335785097537576\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "model = model.to('cpu')\n",
    "\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "Hyper-parameter Setting:  \n",
    "Epoch = 5  \n",
    "Batch size = 8  \n",
    "Learning Rate = 1e-5  \n",
    "Optimizer = Adam  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d670a17cfd384f49b7bc410081eddcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9fd2d4b257448786dd5b83fc6f9f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54c5b2aebff4cb6869c496f9871b6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3909), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] loss: 145.861, training acc: 0.962, dev acc: 0.955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f108973e68fa4dba8e2d4c8899600ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0981c989afc8403a82bf6231f8d900f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db19bbbcf2c4033910beccfeab3b86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3909), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 2] loss: 92.131, training acc: 0.947, dev acc: 0.931\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63c1efb3fd843cfaedf6c6cabe1d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a580b5c396b34e0ea90517f0d622dfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0393414751c6465689696f88a1ab36ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3909), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 3] loss: 71.887, training acc: 0.956, dev acc: 0.935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82634b5cb02646069b76ff62d5dcaed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c211f9d9b0947f38c2782eb747a4164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9685b824a23b43cf992d90dbe3862fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3909), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 4] loss: 55.101, training acc: 0.979, dev acc: 0.953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18f2c764af2457ca91860509e28d295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d26f8e52af4b55874858eacb320901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27de52684b6745ebbe53323048aa553e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3909), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 5] loss: 44.088, training acc: 0.985, dev acc: 0.953\n",
      "Wall time: 49min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()           # training mode\n",
    "\n",
    "# Set Adam Optimizer to upgrade the parameters of the classifier.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in tqdm(trainloader):\n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # Reset gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss of current batch\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        del data\n",
    "        \n",
    "    # Calc the accuracy of classification\n",
    "    model.eval()\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    _, v_acc = get_predictions(model, devloader, compute_acc=True)\n",
    "    model.train()\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, training acc: %.3f, dev acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc, v_acc))\n",
    "    \n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up GPU memory\n",
    "# del outputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ea38137634b769a4d2fae762f053d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3909), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9503693517955933"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "predictions, acc = get_predictions(model, testloader, compute_acc=True)\n",
    "model = model.to('cpu')\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
